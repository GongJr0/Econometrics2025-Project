\section{Results}\label{sec:results}
\subsection{Design Matrix}\label{subsubsec:design_matrix}
First step of the design matrix creation is the identification of optimal $\{p, \ q\}$ parameters for the model.

\subsubsection{Autoregressive Order}\label{subsubsec:ar_order}
Application of ACF yielded a very common pattern of monotonically decreasing correlations, not outlining any specific lag order as superior.
Looking for more conclusive insights, PACF was applied.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{fig/acf_pacf}
    \caption{ACF and PACF plots of the dependent (target) variable.}
    \label{fig:acf_pacf}
\end{figure}

Figure~\ref{fig:acf_pacf} clearly shows the aforementioned decay in ACF\@. 
Conversely, PACF shows lag-1 is the only entry with actual partial contribution into the descriptive structure of $y$.\footnote{The first spike in both ACF and PACF refers to lag-0 (the variable itself) which is equal to 1 by definition.}
Interestingly, the small spikes around lag-20 of the PACF plot (corresponding to 20 quarters or 5 years) indicate that the COVID-19 shock still has a minor effect on the 2025 Real GDP\@.

\vspace{\baselineskip}

In any case, the visualization of the autocorrelation structure presents strong evidence towards the $p=1$ selection, which is used for the model generation in this paper.

\subsubsection{Exogenous Order}\label{subsubsec:exog_order}
The exogenous order selection process started with the visualization of the simple correlation vector $y$ and the lags of exogenous variables.

\begin{table}[ht]
\label{tab:exog_corr}
    \centering
    \begin{tabular}{ccc}
        \toprule
        $\mathbf{i_{lag}}$ & $\mathbf{corr(y, \ CPI_{t-i})}$ & $\mathbf{corr(y, \ u_{t-i})}$ \\
        \midrule
        0 & 0.989 & -0.221 \\
        1 & 0.988 & -0.255 \\
        2 & 0.988 & -0.221 \\
        3 & 0.988 & -0.190 \\
        4 & 0.988 & -0.236 \\
        5 & 0.988 & -0.205 \\
        \bottomrule
    \end{tabular}
    \caption{Pearson correlation coefficients between the dependent variable and lags of exogenous variables.}
\end{table}

Inspecting the table of coefficients, we see that $CPI$ consistently stays at a coefficient of 0.988\@. This indicates that the autoregressive process of $CPI$ is likely very strong, and related to the autoregressive process explaining $y$.
On the other hand, $u$ shows a more reasonable set of coefficients. Although coefficients of $u$ are also relatively stable, they reside in the reasonable range of $[-0.255, \ -0.190]$.
However, a lack of decay or noticeable regime changes in the matrices leaves this simple test inconclusive.
This inconclusiveness was expected, and the CCF plots of whitened variables were examined for clearer insights.

\begin{figure}[ht]
\label{fig:ccf}
    \centering
    \includegraphics[width=\textwidth]{fig/ccf}
    \caption{CCF plots of the whitened exogenous variables and the dependent variable.}
\end{figure}

Although not as clear as the PACF results, Figure~\ref{fig:ccf} carries much stronger insights into the cross-correlation structure compared to raw correlation coefficients.
$CPI$ shows several spikes above the significance bounds with notable lags being $\{0, 45, 47\}$.
For $u$, we observe that lag-0 is the only significant entry.
Based on the observations, lag-0 is the only selection that ensures significant lags of both exogenous variables are included.
Therefore, the cross-correlation analysis yields $q=0$ to be the optimal selection.\footnote{In Equation~\ref{eq:model_eq}, $q=0$ would mean not including exogenous variables. But as a lag order selection, it should be interpreted as using the current observations as regressors.}

\vspace{\baselineskip}
Using $q=0$ introduces the problem of \textbf{data leakage}, as current quarter values of exogenous variables would not be available at the time of estimation.
This results in a design choice of either using the closest available selection in time (i.e.\ lag-1) or in significance (selection of lag-45 or lag-47 for $CPI$).
Since introducing 45 or 47 more coefficients per variable would essentially guarantee overfitting, the final decision was in favor of using $q=1$.\footnote{Further regularization techniques, or using sparse lagged estimators are indeed possible but are deemed outside the scope of this paper.}

\subsubsection{ARX Equation}\label{subsubsec:arx_equation}
With the order specification finalized, we were left with an $ARX(1,\ 1)$ model.
The objective equation is as follows:
\begin{equation}
    \label{eq:arx}
    y_{t} = \beta_0 + \beta_{1} y_{t-1} + \beta_{2} CPI_{t-1} + \beta_{3} u_{t-1} + \varepsilon_{t}
\end{equation}

\noindent At this stage, all model variables were known, and rescaling was performed to standardize the data.
With a concrete design matrix, a final check of VIF between the two exogenous variables was performed to confirm the model parameters did not yield multicollinear exogenous components.
$VIF_{CPI, u}$ was found to be 1.023, indicating near-perfect orthogonality between exogenous regressors. 
Moreover, after standardization, we computed $\operatorname{cond}(X) = 14.75$.
The VIF and condition number both indicate a relatively well-conditioned design matrix.

\subsubsection{Variable Distribution Equality}\label{subsubsec:var_analysis}
The standardized design matrix was used to conduct KS tests to confirm the equality of distributions.
For demonstration purposes, both a simple KS test, and a bootstrapped test with 1,000 iterations were performed.

\begin{figure}[H]
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/ks}
        \caption{KS Test Matrix}
        \label{fig:ks_matrix}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/ks_boot}
        \caption{Bootstrapped KS Test Matrix}
        \label{fig:bootstrapped_ks_matrix}
    \end{minipage}
\end{figure}

We observe that the only difference in results between the two tests is in the $\{y_{t-1}, \ u_{t-1}\}$ pair.
Both tests use $H_0$: $F_x = F_y$ therefore failing to reject means there was not enough evidence to refute the equality of distributions.
However, it is important to note that these tests are not proof of equality, but rather an indication that the distributions are similar enough to not be statistically different.
In this paper, we proceed respect the results of the bootstrapped test (Figure~\ref{fig:bootstrapped_ks_matrix}) and assume equality of distributions for all variable pairs.

\vspace{\baselineskip}

\noindent\textbf{NOTE: } The p-values annotated in the Standard KS matrix are approximations derived by a 101-term Taylor expansion. (Expansion is ill-defined at $D = 0$ for an even number of terms) The test results are evaluated by a comparison of statistics and are not supposed to yield concrete p-values.

\subsection{Model Fit}\label{subsubsec:model_fit}
The fit was performed using Ordinary Least Squares (OLS) on the matrix expansion of Equation~\ref{eq:arx}.
The estimation yielded the following coefficients:
\begin{table}[ht]
    \centering
    \begin{tabular}{cc}
        \toprule
        \textbf{Variable} & \textbf{Coefficient} \\
        \midrule
        $C$ ($\beta_0$) & 16,257.27 \\
        $y_{t-1}$ ($\beta_1$) & 3,795.25 \\
        $CPI_{t-1}$ ($\beta_2$) & 14.83 \\
        $u_{t-1}$ ($\beta_3$) & 65.54 \\
        \bottomrule
    \end{tabular}
    \caption{Estimated coefficients of the ARX(1, 1) model.}
    \label{tab:estimated_coefficients}
\end{table}

With a normalized design matrix, the intercept column expectedly has to take on the re-scaling of inputs.
Also, the strong influence of $y_{t-1}$ is inline with the autocorrelation analysis in Section~\ref{subsubsec:ar_order}.
The only notable observation is the positive coefficient of $u_{t-1}$, which both by theory and correlation analysis should have been a negative value.
This anomaly is likely attributable to the normalization step introducing negative values into an otherwise positive-only variable.

\vspace{\baselineskip}
The overall model fit statistics are as follows:
\begin{figure}[H]
    \centering
    \begin{minipage}{0.35\textwidth}
        \centering
        \begin{tabular}{cc}
            \toprule
            \textbf{Statistic} & \textbf{Value} \\
            \midrule
            $R^2$ & 0.9972 \\
            Adjusted $R^2$ & 0.9971 \\
            RMSE & 201.30 \\
            MAPE & 0.62\% \\
            F-statistic & 12,002.50 \\
            p-value ($F$) & $\approxeq 0$ \\
            $X^\top \hat{\varepsilon}$ & $-6.25 \times 10^{-9} \approxeq 0$ \\
            \bottomrule
        \end{tabular}
        \caption{Overall model fit statistics.}
        \label{tab:model_fit_statistics}
    \end{minipage}
    \hfill
    \begin{minipage}{0.55\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/fit_v_obs}
        \caption{Fitted vs. Observed Values}
        \label{fig:fitted_vs_observed}
    \end{minipage}
\end{figure}

All metrics indicate a good fit to in-sample data.
However, the extremely high $R^2$ and F-statistic are clear signs of overfitting.
Overall, the fit behavior indicates a model where coefficients are interpretable, but predictions should be handled with care.
Figure~\ref{fig:fitted_vs_observed}, displays 2 expected locations where residuals noticeably deviate from 0:
\begin{itemize}
    \item The 2008 Financial Crisis
    \item The 2020 COVID-19 Shock
\end{itemize}

As expected from a generalized model, the shocks cause an increase in error.
However, it's arguable that the model still responds to the shocks more than what would be expected from a well-generalized estimator.
With the highly autocorrelated $y$ and the presence of $y_{t-1}$ in the design matrix, the unregularized $ARX$ model expectedly "follows" the shocks.
In fact, despite the reasonable fit statistics, the use of \textbf{Ridge} or \textbf{Lasso} regularization would yield better generalization for the tradeoff of a slightly reduced $R^2$.

\vspace{\baselineskip}

To test the predictive performance of the model, a quick test with the only available out-of-sample data (Q2 2025) was conduicted. A prediction of $\$23,632.14$ Bn was achieved against a true value of $\$23,770.98$ Bn.
This results in a percentage error of $-0.58$\% which is within the expected range by MAPE.

\subsection{Residual Analysis}\label{subsubsec:residual_analysis}
Given the fit information above, we can safely expect well-behaved residuals. 
However, a thorough analysis of residuals is of course necessary to confirm the validity of OLS assumptions. 
Plotting residuals over time presents the opportunity to inspect the exact magnitudes of the errors in the shock periods discussed in Section~\ref{subsubsec:model_fit}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.5\textwidth]{fig/resid}
    \caption{Residuals over time.}
    \label{fig:residuals_time}
\end{figure}

Figure~\ref{fig:residuals_time} outlines two abnormalities in residuals using a 0.99 confidence interval to define outliers.
Both extremes occur in the COVID-19 period, but looking at the 2008 period, we clearly see an overestimation of $y$.
Inspecting the line of best fit shows us that the residuals are centered around 0, with a slope of 0.
The residuals are heavily concentrated around 0, indicating a tighter empirical distribution compared to a Normal distribution.
Fitting a Normal and t-student distribution to the residuals confirms this observation.
\begin{figure}[H]
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/resid_dist}
        \caption{Residual density with fitted distributions.}
        \label{fig:residuals_distribution}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/qq}
        \caption{QQ Plot of Residuals vs. $N(0, 1)$ quantiles.}
        \label{fig:qq_plot}
    \end{minipage}
\end{figure}

The QQ plot of residuals against the Normal distribution (Figure~\ref{fig:qq_plot}) 
shows that most observations lie on a line with a noticeably smaller slope than the 
reference line $\bar y + S_y x_i$, indicating that the bulk of the residuals is less 
dispersed than a Normal distribution with the same mean and standard deviation. 
However, there are extreme points in both tails that lie well outside the 95\% 
simulation envelope, reflecting occasional large shocks. 
Together with the fitted Student-$t$ distribution with $\widehat{\mathrm{df}} \approx 2.36$, 
this suggests a very concentrated core with heavier-than-Normal tails rather than 
Gaussian residuals.
As an added confirmation, the Shapiro--Wilk test rejects normality with $W=0.62$ and $p = 1.81 \times 10^{-17} \approxeq 0$.

\vspace{\baselineskip}

As a final descriptor of residual behavior, we inspect the stationarity using the ADF test. 
Knowing the residuals are zero-centered, we fit an ADF regression with no deterministic terms.
with $ADF = -2.25$ we obtain a $p$-value of 0.02 and reject the null hypothesis of a unit root at the 5\% significance level.
This confirms the stationarity of residuals and presents further evidence towards homoskedasticity.

\subsection{Gauss\rule[0.5ex]{0.5em}{0.7pt}Markov Conditions}\label{subsubsec:gm_conditions}

Although we present strong evidence towards all Gauss-Markov conditions being satisfied, we further confirm our findings using the tests outlined in Section~\ref{subsec:blue_evaluation}.

\begin{itemize}
    \item \textbf{Linearity:} The model is linear in parameters by design (Equation~\ref{eq:arx} \& Equation~\ref{eq:model_eq}).
    \item \textbf{Full Rank:} We computed a lack of perfect multicollinearity with $VIF_{CPI, u} = 1.023 < 5$ and $\operatorname{cond}(X) = 14.75 < 30$
    \item \textbf{Strict Exogeneity:} We computed $X^\top \hat{\varepsilon} = -6.25 \times 10^{-9} \approxeq 0$ which shows no correlation between the regressors and residuals.
    \item \textbf{Spherical Errors:} Results of the BP test yielded $BP=3.88$ and $p=0.73$ failing to reject the null hypothesis of homoskedasticity. 
    Additionally, BG tests for lags $[1, 8]$ (2 years) all failed to reject the null hypothesis of no autocorrelation with the most significant result being $BG(7)=6.14$ and $p=0.48$; while the least significant was $BG(3)=5.19$ and $p=0.61$.
\end{itemize}