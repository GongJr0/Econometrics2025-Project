\section{Methodology}\label{sec:methodology}

The proposed methodology of this paper can be split into three main components:
\begin{itemize}
    \item Data Preparation and Examination
    \item Model Estimation 
    \item BLUE Evaluation
\end{itemize}

Each component is detailed in the following subsections.

\subsection{Data Preparation and Examination}\label{subsec:data_prep_exam}
Data preparation does not involve any steps that are specific to a given column of the dataset. Therefore, $X_{i,j}$ notation will be used to denote any given column/observation and $\beta_i$ will be used to denote any given coefficient.
In addition to formatting the $X$ matrix to match the model structure, the preparation steps emphasize transforming the raw data for interpretability while preserving its underlying distributions.

\subsubsection{Dataset Structure Validation}\label{subsubsec:inp_val}
More often than not, the performance of a non-robust, linear estimator is governed by the quality of the input variables.
Ideally, we look for a dataset that's \textbf{free of multicollinearity, extreme outliers, and missing values} while being \textbf{correlated to the target}.
This general construction achieves a model where all variables have uniquely identifiable contributions to the output, often leading to lower variances and better predictive power.

\vspace{\baselineskip}

Autoregressive models present a challenge in this regard since variables selected to be correlated to the target ($y$) are also correlated to the lagged representation of it within $X$.
Acknowledging this issue, the endogenous portion of the model will only be checked to ensure there are no singularities by perfect correlation inside $X$. 
The presence/absence of multicollinearity will be displayed through a correlation heatmap.

\vspace{\baselineskip}

Furthermore, data continuity will be prioritized over outlier removal since the model itself is time-dependent.
Outliers, if any, will be addressed if they are found to be damaging to the model's performance after the initial estimation.

\vspace{\baselineskip}

Finally, any missing values will be handled through linear interpolation of the observations prior and after the occurrence.


\subsubsection{Design Matrix Formation}\label{subsubsec:inp_mat}
$X$ is formed by converting the model equation specified in Equation~\ref{eq:model_eq} into matrix form.
The inner product operation specified as $ARX(p, \ q)$ can be written as a matrix multiplication of observations and coefficients:

\begin{equation}
    \label{eq:des_mat}
    \begin{bmatrix}
        1 & X_{1,1} & \cdots &  X_{1,m} \\
        \vdots  & \vdots & \ddots &  \vdots \\
        1 & X_{n,1} & \cdots & X_{n,m}
    \end{bmatrix}
    \times
    \begin{bmatrix}
        C \\
        \beta_1 \\
        \vdots \\
        \beta_{m}
    \end{bmatrix}
\end{equation}

Where $n$ is the number of observations and $m$ is the number of features (including the intercept) dictated by the $\{p, \ q\}$ orders of the model.

\vspace{\baselineskip}
As shown in the above equation, a closed form matrix representation of the model requires $p$ and $q$ to be determined.
In this paper, the selection of these hyperparameters will follow the simple and idealistic approach of selecting the lowest possible orders that lead to an efficient estimator.

\subsubsection{Model Order Selection}\label{subsubsec:inp_order}
First step of the design matrix creation is the identification of optimal $\{p, \ q\}$ parameters for the model.
There are two distinct methods to select the autoregressive and exogenous lag order of the model:

\begin{itemize}
    \item \textbf{Autoregressive Order} \textemdash~\enquote{Autocorrelation Function} (ACF) and \enquote{Partial Autocorrelation Function} (PACF) plots of the dependent variable are examined to identify significant lags. The ACF plot helps to identify the overall correlation structure, while the PACF plot isolates the direct effect of each lag.
    \item \textbf{Exogenous Order} \textemdash~ Lagged correlation matrices and \enquote{Cross-Correlation Function} (CCF) plots between the dependent variable and each exogenous variable are analyzed to determine significant lags. The CCF plot reveals the correlation between the dependent variable and lagged values of the exogenous variables.
\end{itemize}

While ACF and PACF can directly be applied to the dependent variable, CCF results can be heavily distorted by nonstationary and the presence of AR structures in the variables being compared.
Therefore, the \textbf{Box\rule[0.5ex]{0.5em}{0.7pt}Jenkins Prewhitening} procedure will be applied to expose a clearer signal of the underlying cross-correlations.
The procedure transforms the exogenous variables into residuals of their $AR$ representation, then uses the $\beta$ coefficient to re-scale the dependent variable. (\cite{hackhard_9_2025})
For our purposes, the process will be applied to normalized data, making the inclusion of an intercept redundant.
Formally, the $AR(1)$ variant of the process can be shown as:

\begin{equation}
    \label{eq:whitening}
    \begin{aligned}
        \hat{X_i} &= (L^1X_i) \beta_i \\
        \hat{y_i} &= y \beta_i \\
        \rho_{y,i} &= \operatorname{corr}(\hat{y_i}, X_i - \hat{X_i})
    \end{aligned}
\end{equation}
\vspace{\baselineskip}

\noindent Where $L^1$ is the lag-1 operator.

\vspace{\baselineskip}

Following this selection procedure, the candidate $\{p, \ q\}$ are used to form a design matrix for the final confirmation of the \textbf{Variance Inflation Factor} (VIF) between exogenous variables.
VIF is computed as:
\begin{equation}
    \label{eq:vif}
    VIF_{X_i} = \frac{1}{1 - R^2_{X_i}}
\end{equation}

\noindent Where $R^2_{X_i}$ is the coefficient of determination of the regression of $X_i$ on all other columns of $X$.
In our case, a lag of the target variable is known to be present in $X$.
As exogenous variables are selected to explain the variance in $y$, it is expected that they will attain a high $R^2$ as regressors of $y_{t-i}$.
Referring back to Equation~\ref{eq:vif}, we observe that this phenomenon will introduce artificially inflated VIF calculations as $R^2$ acts as a deflator of the denominator.
Therefore, VIF calculations will only be used to calculate the inflation factor between exogenous variable (via a reduced design matrix $X^{(exog)}$), confirming the two variables are not explaining the same portion of variance in $y$.\footnote{See Section~\ref{subsec:remark_vif} for further discussion on the VIF application in this context.}

\subsubsection{Rescaling}\label{subsubsec:inp_rescale}
Rescaling the input matrix ($X$) allows better comparison of coefficients and ensures that $\sum X^\top\varepsilon = 0$ must hold within very tight tolerances\footnote{Differences in magnitude between $X$ columns can lead to numerical ambiguity in orthogonality checks. It is generally preferable to eliminate any differences in the order of magnitudes in $X$.}.
A common approach to achieve this stability is standardization, which transforms each feature to have a mean of 0 and a standard deviation of 1.
Standardization is performed column-wise on the input using the formula:
\begin{equation}
    \label{eq:z_formula}
    Z = \frac{X_{i, j} - \bar{X_i}}{S_i}    
\end{equation}



\noindent Where $\bar{X_i}$ is the mean of the $i$-th column, and $S_i$ is the [unbiased] standard deviation of the $i$-th column.

\vspace{\baselineskip}
This transformation has no effect on the error profile of the fitted model as the distribution shapes and inter-variable interactions are perfectly preserved.\footnote{The statement holds for any non-penalized estimator of the form $\hat\beta = \arg\min_\beta L(y, X\beta)$, since rescaling $X$ can be absorbed into $\beta$.}
However, the transformation brings all variables to the same scale and units.
As a result, the \textbf{Kolmogorov\textbf{\rule[0.5ex]{0.5em}{0.7pt}}Smirnov (KS) test} which is sensitive to such differences now becomes available.

\subsubsection{Analysis of Normalized Input Distributions}\label{subsubsec:inp_distr}
With each regressor standardized to zero mean and unit variance, a one unit change corresponds to a $1\sigma$ shift in the original scales.
However, this unit equality does not make the "likelihood" (or "extremeness") of different variables comparable.
The "Z-scores" are comparable in likelihood only if the variables subject to comparison are identically distributed.
This part of the methodology works towards identifying such equalities in $X$.

\vspace{\baselineskip}

Concretely, the aforementioned KS test can be used to check for equality of distributions.
However, the KS' assumption of \textbf{i.i.d.\@ samples} is virtually impossible to satisfy in time-series data.
To address this, a modification of the \textbf{Bootstrap KS Test} (\cite{praestgaard_permutation_1995}) can be applied.

\vspace{\baselineskip}

The specific test statistic is computed by calculating
 
\[D_{i,j} = \sup_x |F_{X_i}(x) - F_{X_j}(x)|\] 

\noindent Where $F_{X_i}$ and $F_{X_j}$ are the Empirical CDFs of the respective regressors. 
Afterward, a comparison of the bootstrapped distribution of $D_{i,j}^*$ to the observed $D_{i,j}$ is performed.

\vspace{\baselineskip}

The empirical p-value is estimated as the proportion defined by:

\begin{equation}
    \label{eq:ks_pval}
    p_{i,j} = \frac{1}{B+1} \bigg(\sum_{b=1}^{B} (\mathbbm{1}_{\{D_{i,j}^{*(b)} \geq D_{i,j}\}}) + 1\bigg)
\end{equation}

\noindent Where $B$ is the number of bootstrap samples. 
To preserve the in-block serial dependence of observations, we employ a \textbf{Block Bootstrap} approach instead of the standard bootstrap resampling.\footnote{See Section~\ref{subsec:remark_ks} for further discussion on the time-series application of the KS test.}
\subsection{Model Estimation}\label{subsec:model_estimation}
The model estimation is conducted using the standard \textbf{Ordinary Least Squares} (OLS) approach.
Using the derived design matrix $X$ we minimize:
\begin{equation}
    \label{eq:ols_obj}
    \begin{aligned}
        J(\beta) & = ||y - X\beta||^2_2 \\
        & = (y - X\beta)^T(y - X\beta)
    \end{aligned}
\end{equation}

\noindent Which transforms into the first-order condition:
\begin{equation}
    \label{eq:ols_foc}
    \begin{aligned}
        X^\top\hat{\beta} & = X^\top y \\
        \hat{\beta} & = (X^\top X)^{-1} X^\top y
    \end{aligned}
\end{equation}

\noindent Where $\hat{\beta}$ is the vector of estimated coefficients.
This gives us a closed-form, single-step solution to the estimation problem.

\subsection{BLUE Evaluation}\label{subsec:blue_evaluation}
After estimating the model coefficients, we evaluate the Gauss--Markov conditions to confirm whether the resulting fit is BLUE.
Previous stages of the methodology involve checks for the pre-fit Gauss--Markov conditions:
\begin{itemize}
    \item \textbf{Linearity of Parameters:} by definition the model employed is a linear combination of regressors and coefficients.
    Therefore, this condition is satisfied at the stage of model selection.
    \item \textbf{Full Rank:} $rank(X)=k$ must hold for $X^\top X$ to be invertible. 
    Therefore, this condition must hold for the model to be estimable via OLS.
\end{itemize}

\noindent The remaining Gauss--Markov conditions concern $\varepsilon$ and are verified post-estimation:

\begin{itemize}
    \item \textbf{Strict Exogeneity:} Verified by checking that $\sum_{i=1}^n X_i\varepsilon_i = 0$ (or $X^\top \varepsilon = 0$)  holds within tight tolerances.
    \item \textbf{Spherical Errors:} This condition holds if $\operatorname{Var}(\varepsilon) = \sigma^2 I_n$.
    If residuals are homoskedastic and uncorrelated, this condition is satisfied. The two criteria can be tested separately using the \textbf{Breusch\rule[0.5ex]{0.5em}{0.7pt}Pagan Test} for homoskedasticity and the \textbf{Breusch\rule[0.5ex]{0.5em}{0.7pt}Godfrey Test} for autocorrelation.
\end{itemize}

Noticeably, all conditions except spherical errors can be verified through deterministic calculations using the information attained at-or-before estimation.
Fittingly, the tests involved in verifying spherical errors are relatively simple and lead to intuitive test statistics.

\vspace{\baselineskip}

The Breusch--Pagan test checks for heteroskedasticity by first deriving the residual based dependent variable $g$:

\begin{equation}
    \label{eq:bp_aux}
    \begin{aligned}
        \varepsilon^2 &= (y-X\hat{\beta})^2 \\
        \hat{\sigma}^2 &= \frac{\sum_{i=1}^n \varepsilon_i^2}{n} \\
        g &= \frac{\varepsilon^2}{\hat{\sigma}^2}
    \end{aligned}
\end{equation}

\noindent Where $\hat{\sigma}^2$ is the mean of squared residuals. Therefore, $g$ is a mean-scaled vector of squared errors.
Afterward, an auxiliary regression of $g$ on $X$ is performed to attain the $R^2$ of the fit. The test statistic is then computed:
\begin{equation}
    \label{eq:bp_stat}
    nR^2(g, \ \hat{g}) \sim \chi^2_{k-1}
\end{equation}
\noindent Where $k$ is the number of regressors (including the intercept).

\vspace{\baselineskip}
The Breusch--Godfrey test creates a very similar statistic, again derived from a regression of residuals.
The test is performed by estimating an $AR(p)$ model of the residuals. The statistic is again derived from $R^2$ with only a slight modification:
\begin{equation}
    \label{eq:bg_stat}
    (n-p)R^2(\varepsilon, \ \hat{\varepsilon}) \sim \chi^2_{p}
\end{equation}

\noindent Where $p$ ($=k-1$ from the BP test) is the number of lags used in the auxiliary regression.