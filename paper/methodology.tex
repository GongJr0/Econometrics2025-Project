\section{Methodology}\label{sec:methodology}

The proposed methodology of this paper can be split into three main components:
\begin{itemize}
    \item Data Preparation and Examination
    \item Model Estimation 
    \item BLUE Evaluation
\end{itemize}

Each component is detailed in the following subsections.

\subsection{Data Preparation and Examination}\label{subsec:data_prep_exam}
Data preparation does not involve any steps that are specific to a given column of the dataset. Therefore, $X_{i,j}$ notation will be used to denote any given column/observation and $\beta_i$ will be used to denote any given coefficient.
In addition to formatting the $X$ matrix to match the model structure, the preparation steps emphasize transforming the raw data for interpretability while preserving its underlying distributions.

\subsubsection{Dataset Structure Validation}\label{subsubsec:inp_val}
More often than not, the performance of a non-robust, linear estimator is governed by the quality of the input variables.
Ideally, we look for a dataset that's \textbf{free of multicollinearity, extreme outliers, and missing values} while being \textbf{correlated to the target}.
This general construction achieves a model where all variables have uniquely identifiable contributions to the output, often leading to lower variances and better predictive power.

\vspace{\baselineskip}

Autoregressive models present a challenge in this regard since variables selected to be correlated to the target ($y$) are also correlated to the lagged representation of it within $X$.
Acknowledging this issue, the endogenous portion of the model will only be checked to ensure there are no singularities by perfect correlation inside $X$. 
The presence/absence of multicollinearity will be displayed through a correlation heatmap.

\vspace{\baselineskip}

Furthermore, data continuity will be prioritized over outlier removal since the model itself is time-dependent.
Outliers, if any, will be addressed if they are found to be damaging to the model's performance after the initial estimation.

\vspace{\baselineskip}

Finally, any missing values will be handled through linear interpolation of the observations prior and after the occurrence.


\subsubsection{Design Matrix Formation}\label{subsubsec:inp_mat}
$X$ is formed by converting the model equation specified in Equation~\ref{eq:model_eq} into matrix form.
The inner product operation specified as $ARX(p, \ q)$ can be written as a matrix multiplication of observations and coefficients:

\begin{equation}
    \label{eq:des_mat}
    \begin{bmatrix}
        1 & X_{1,1} & \cdots &  X_{1,m} \\
        \vdots  & \vdots & \ddots &  \vdots \\
        1 & X_{n,1} & \cdots & X_{n,m}
    \end{bmatrix}
    \times
    \begin{bmatrix}
        C \\
        \beta_1 \\
        \vdots \\
        \beta_{m}
    \end{bmatrix}
\end{equation}

Where $n$ is the number of observations and $m$ is the number of features (including the intercept) dictated by the $\{p, \ q\}$ orders of the model.

\vspace{\baselineskip}
As shown in the above equation, a closed form matrix representation of the model requires $p$ and $q$ to be determined.
In this paper, the selection of these hyperparameters will follow the simple and idealistic approach of selecting the lowest possible orders that lead to an efficient estimator.

\subsubsection{Model Order Selection}\label{subsubsec:inp_order}
First step of the design matrix creation is the identification of optimal $\{p, \ q\}$ parameters for the model.
There are two distinct methods to select the autoregressive and exogenous lag order of the model:

\begin{itemize}
    \item \textbf{Autoregressive Order} \textemdash~\enquote{Autocorrelation Function} (ACF) and \enquote{Partial Autocorrelation Function} (PACF) plots of the dependent variable are examined to identify significant lags. The ACF plot helps to identify the overall correlation structure, while the PACF plot isolates the direct effect of each lag.
    \item \textbf{Exogenous Order} \textemdash~ Lagged correlation matrices and \enquote{Cross-Correlation Function} (CCF) plots between the dependent variable and each exogenous variable are analyzed to determine significant lags. The CCF plot reveals the correlation between the dependent variable and lagged values of the exogenous variables.
\end{itemize}

While ACF and PACF can directly be applied to the dependent variable, CCF results can be heavily distorted by nonstationary and the presence of AR structures in the variables being compared.
Therefore, the \textbf{Box\rule[0.5ex]{0.5em}{0.7pt}Jenkins Prewhitening} procedure will be applied to expose a clearer signal of the underlying cross-correlations.
The procedure transforms the exogenous variables into residuals of their $AR$ representation, then uses the $\beta$ coefficient to re-scale the dependent variable. (\cite{hackhard_9_2025})
For our purposes, the process will be applied to normalized data, making the inclusion of an intercept redundant.
Formally, the $AR(1)$ variant of the process can be shown as:

\begin{equation}
    \label{eq:whitening}
    \begin{aligned}
        \hat{X_i} &= (L^1X_i) \beta_i \\
        \hat{y_i} &= y \beta_i \\
        \rho_{y,i} &= \operatorname{corr}(\hat{y_i}, X_i - \hat{X_i})
    \end{aligned}
\end{equation}
\vspace{\baselineskip}

\noindent Where $L^1$ is the lag-1 operator.

\subsubsection{Rescaling}\label{subsubsec:inp_rescale}
Rescaling the input matrix ($X$) allows better comparison of coefficients and ensures that $\sum X^T\varepsilon = 0$ must hold within very tight tolerances\footnote{Differences in magnitude between $X$ columns can lead to numerical ambiguity in orthogonality checks. It is generally preferable to eliminate any differences in the order of magnitudes in $X$.}.
A common approach to achieve this stability is standardization, which transforms each feature to have a mean of 0 and a standard deviation of 1.
Standardization is performed column-wise on the input using the formula:
\begin{equation}
    \label{eq:z_formula}
    Z = \frac{X_{i, j} - \bar{X_i}}{S_i}    
\end{equation}



\noindent Where $\bar{X_i}$ is the mean of the $i$-th column, and $S_i$ is the [unbiased] standard deviation of the $i$-th column.

\vspace{\baselineskip}
This transformation has no effect on the error profile of the fitted model as the distribution shapes and inter-variable interactions are perfectly preserved.\footnote{The statement holds for any non-penalized estimator of the form $\hat\beta = \arg\min_\beta L(y, X\beta)$, since rescaling $X$ can be absorbed into $\beta$.}
However, the transformation brings all variables to the same scale and units.
As a result, the \textbf{Kolmogorov\textbf{\rule[0.5ex]{0.5em}{0.7pt}}Smirnov (KS) test} which is sensitive to such differences now becomes available.

\subsubsection{Normalized Input Distributions}\label{subsubsec:inp_distr}
As discussed, the rescaling step allows the application of the KS test; which checks the equality of distributions between the inputs. 
Two variables with equality in location, scale, and shape are directly comparable.
The Z-transform ensures equality in scale and location. 
Therefore, the variable pairs where a 2-sample KS test finds no grounds to reject $H_0$ satisfy all three conditions of equality.

\vspace{\baselineskip}
For variables that satisfy the conditions, a shared z-value indicates the observations were equally likely to be drawn.
This fact allows the direct comparison of coefficients as $X_{i, j} \beta_i$ and $X_{k, j} \beta_k$ are only differentiated by their respective $\beta$.
\subsection{Model Estimation}\label{subsec:model_estimation}
\dots

\subsection{BLUE Evaluation}\label{subsec:blue_evaluation}
\dots